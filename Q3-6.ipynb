{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a5beb9-58d3-4352-8a50-29afbbe9da2e",
   "metadata": {},
   "source": [
    "**VARSHA SEEMALA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f91647-2c44-420c-905a-abbec23e89c4",
   "metadata": {},
   "source": [
    "**23B0964**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448219d0-38cd-45e8-bd90-f57a035d2eb9",
   "metadata": {},
   "source": [
    "**QUESTION 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f23c29-c76f-4e1a-99b5-54a0e87545a2",
   "metadata": {},
   "source": [
    "Here we have an existing database of 8000 customers(Consumer_Dataset.csv) and another database containing information about 2500 potential new customers(Consumer_Test_Dataset.csv).We have to implement a method to predict the group(A,B,C or D) of the customers in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81b331-79a7-49c9-a483-a925f7cc6cd5",
   "metadata": {},
   "source": [
    "We can implement a machine learning approach to predict customer segments (A, B, C, or D) for new potential customers in the Indian market. The approach leverages data from an existing market to train the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbade355-6469-4213-8d66-8ff9219cf4d3",
   "metadata": {},
   "source": [
    "Let us look into the methodolgy we have used here:\n",
    "\n",
    "1. Data Loading and Preprocessing:\n",
    "   \n",
    "   -->Missing values are handled strategically:\n",
    "   \n",
    "   Categorical: Missing values in categorical columns (e.g., Ever_Married, Profession) are filled with the most frequent category (mode) from the training data.\n",
    "   \n",
    "   Numerical: Missing values in numerical columns (e.g., Family_Size, Work_Experience) are filled with the median value from the training data.\n",
    "\n",
    "2. -->Categorical features are converted into numerical format using Label Encoding. This is necessary because most machine learning models work with numerical data.\n",
    "   \n",
    "   -->Numerical features are standardized (mean of 0, standard deviation of 1) using StandardScaler. This ensures that all features contribute equally to the model's learning process, regardless of their original scales.\n",
    "   \n",
    "3. Model Selection and Training:\n",
    "   \n",
    "   -->Random Forest Classifier: We have used a Random Forest classifier learning method that combines multiple decision trees to make predictions.  Random Forests are known for their robustness, ability to handle various types of data, and resistance to overfitting.\n",
    "   \n",
    "   -->Hyperparameter Tuning: The code employs GridSearchCV to find the best combination of hyperparameters for the Random Forest model.Grid search systematically tries different combinations of hyperparameters and evaluates the model's performance using cross-validation. The best set of hyperparameters is then used to train the final model.\n",
    "   \n",
    "4. Prediction and Output:\n",
    "\n",
    "   -->Prediction on Test Data: The best model obtained from grid search is used to predict the customer segment (predicted_group) for each customer in the test dataset.\n",
    "   \n",
    "   -->Handling Invalid Predictions: The code includes a step to ensure that any predictions that are not within the valid group labels (A, B, C, D) are replaced with 'D'. This is a safety measure to handle any unexpected outputs from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df83780-40ff-4797-a5ba-9b8bb518513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.46      0.48      0.47       391\n",
      "           B       0.47      0.34      0.39       369\n",
      "           C       0.54      0.58      0.56       380\n",
      "           D       0.66      0.75      0.70       474\n",
      "\n",
      "    accuracy                           0.55      1614\n",
      "   macro avg       0.53      0.54      0.53      1614\n",
      "weighted avg       0.54      0.55      0.54      1614\n",
      "\n",
      "Confusion Matrix:\n",
      " [[188  57  61  85]\n",
      " [ 86 124 114  45]\n",
      " [ 48  61 222  49]\n",
      " [ 86  22  11 355]]\n",
      "| predicted_group   |\n",
      "|:------------------|\n",
      "| A                 |\n",
      "| B                 |\n",
      "| B                 |\n",
      "| B                 |\n",
      "| D                 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('Consumer_Dataset.csv')\n",
    "df_test = pd.read_csv('Consumer_Test_Dataset.csv')\n",
    "\n",
    "for col in ['Ever_Married', 'Profession', 'Graduated', 'Preferred_Renewable']:\n",
    "    mode_value = df_train[col].mode()[0]\n",
    "    df_test[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "for col in ['Family_Size', 'Work_Experience']:\n",
    "    median_value = df_train[col].median()\n",
    "    df_test[col].fillna(median_value, inplace=True)\n",
    "\n",
    "categorical_columns = ['Gender', 'Ever_Married', 'Profession', 'Graduated', 'Preferred_Renewable', 'Energy_Consumption']\n",
    "for df in [df_train, df_test]:\n",
    "    for column in categorical_columns:\n",
    "        df[column] = df[column].astype(str)\n",
    "\n",
    "label_encoders_train = {}\n",
    "label_encoders_test = {}\n",
    "for column in categorical_columns:\n",
    "    le_train = LabelEncoder()\n",
    "    le_test = LabelEncoder()\n",
    "    df_train[column] = le_train.fit_transform(df_train[column])\n",
    "    df_test[column] = le_test.fit_transform(df_test[column])\n",
    "    label_encoders_train[column] = le_train\n",
    "    label_encoders_test[column] = le_test\n",
    "\n",
    "numeric_columns = ['Age', 'Family_Size', 'Work_Experience']\n",
    "for df in [df_train, df_test]:\n",
    "    for column in numeric_columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "for df in [df_train, df_test]:\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "for df in [df_train, df_test]:\n",
    "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "X_train = df_train.drop('Group', axis=1)\n",
    "y_train = df_train['Group']\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, verbose=0)\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_rf.predict(X_val)\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "predictions = best_rf.predict(df_test)\n",
    "df_test['predicted_group'] = predictions\n",
    "\n",
    "df_test['predicted_group'] = df_test['predicted_group'].apply(lambda x: x if x in ['A', 'B', 'C', 'D'] else 'D')\n",
    "\n",
    "print(df_test[['predicted_group']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04767aaf-e371-43a6-b959-5b041ca9ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Gender       Age  Ever_Married  Family_Size  Profession  \\\n",
      "0           0       0 -0.450948             1    -1.207220           2   \n",
      "1           1       1 -0.391999             1     0.768709           5   \n",
      "2           2       0  1.494372             1    -1.207220           0   \n",
      "3           3       1  0.904881             1    -0.548577           4   \n",
      "4           4       0 -1.453083             0     0.768709           8   \n",
      "5           5       1  0.197492             1     1.427352           1   \n",
      "6           6       1  1.022779             1     0.110066           1   \n",
      "7           7       0  0.197492             1     0.110066           0   \n",
      "8           8       1  0.374339             1     0.768709           0   \n",
      "9           9       1 -1.453083             0     0.768709           5   \n",
      "\n",
      "   Graduated  Work_Experience  Energy_Consumption  Preferred_Renewable  \\\n",
      "0          1        -0.748105                   2                    4   \n",
      "1          1         1.752242                   0                    4   \n",
      "2          0        -0.748105                   2                    4   \n",
      "3          0         2.689872                   1                    4   \n",
      "4          0        -0.435562                   2                    4   \n",
      "5          1        -0.748105                   1                    2   \n",
      "6          1         0.814612                   2                    4   \n",
      "7          1        -0.435562                   0                    4   \n",
      "8          1        -0.123019                   0                    4   \n",
      "9          0        -0.748105                   2                    4   \n",
      "\n",
      "  predicted_group  \n",
      "0               A  \n",
      "1               B  \n",
      "2               B  \n",
      "3               B  \n",
      "4               D  \n",
      "5               B  \n",
      "6               A  \n",
      "7               C  \n",
      "8               C  \n",
      "9               D  \n"
     ]
    }
   ],
   "source": [
    "first_ten_rows = df_test.head(10)\n",
    "print(first_ten_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7ad69-aa76-4896-85c1-f6516046ada6",
   "metadata": {},
   "source": [
    "At the end a classification report is generated which provides a comprehensive summary of the model's performance on the validation set.The model has an accuracy of 55%. While it performs well for class D, there might be some issues with classes A and B. The class imbalance might be a contributing factor to this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203355fd-e483-4581-9650-8ba98a8cf4f6",
   "metadata": {},
   "source": [
    "The reason behind choosing the Random Forests classification is that Random Forests can capture complex, non-linear relationships between features and the target variable (customer segment), making them suitable for segmentation tasks where customer behavior isn't always easily explained by simple rules.It is also less prone to overfitting compared to some other models (e.g., single decision trees) due to their ensemble nature. This means they are more likely to generalize well to new data.Grid-Search also significantly improves the model performance as it explores different combinations of hyperparameters (e.g., number of trees, maximum depth) to find the best model configuration.\n",
    "\n",
    "I have also found about other classification algorithms like logistic regression, support vector machines etc on the internet, but decided to proceed with Random Forests as they are often a good starting point due to their robustness and generally good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c926c23-f756-4bbc-b310-30baeb78789b",
   "metadata": {},
   "source": [
    "__QUESTION 4__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e413ab-752c-414a-bbe9-3a797a1b597c",
   "metadata": {},
   "source": [
    "In the absence of pre-existing customer groups and their corresponding data, we would have to switch from supervised learning  to unsupervised learning. This involves identifying patterns and structures within the data to group customers based on their inherent similarities.\n",
    "\n",
    "I feel that in this scenario Clustering Algorithms would be the best approach.Here are some of the famous clustering algorithms i have found out about on the internet:\n",
    "\n",
    "K-means clustering: This algorithm partitions customers into k clusters based on the similarity of their features. The optimal number of clusters (k) can be determined using techniques like the elbow method or silhouette analysis.\n",
    "Hierarchical clustering: This method builds a hierarchy of clusters, starting with each customer as a separate cluster and progressively merging them based on similarity. This can reveal the underlying hierarchical structure of customer groups.\n",
    "Density-Based Spatial Clustering of Applications with Noise (DBSCAN): This algorithm groups together points that are closely packed, and identifies outliers as noise. It's suitable for datasets with varying density and irregularly shaped clusters.\n",
    "Gaussian Mixture Models (GMM): This probabilistic model assumes that the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. It can handle clusters with overlapping distributions.\n",
    "\n",
    "I would prefer to use the K-means algorithm as it is computationally efficient than the others.\n",
    "After pre-processing the data we can use the Elbow method to determine the optimal number of clusters and apply K-Means clustering to the preprocessed data.K-means requires us to specify the number of clusters (k) beforehand. The Elbow method helps us to determine the optimal value of k by evaluating the model's performance (inertia) for different values of k.\n",
    "Based on the K-Means analysis we can identify distinct customer segments in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f7926-073d-43f0-b4de-f5e4f2c96305",
   "metadata": {},
   "source": [
    "**QUESTION 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a88ad-c7ee-49ad-b3e1-334b65975d19",
   "metadata": {},
   "source": [
    "![IMAGE](Q6.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d866ac-102e-45e4-b14c-00b49786c2d3",
   "metadata": {},
   "source": [
    "The figures above show the performance of the existing model in predicting the number of renewable energy products sold. The left graph displays the predicted vs. actual values over time, and the right graph shows the error (difference between predicted and actual) over time.\n",
    "Below are some potential causes of poor model performance that I feel could be relevant:\n",
    "\n",
    "The model might be using features that are not strongly correlated with renewable energy production. For example, the model might not be considering weather patterns adequately, or it might be overlooking important economic indicators.\n",
    "\n",
    "The model could be too simple to capture the complex relationships between the features and product sales, or it might be too complex, leading to overfitting on the training data. In the case of overfitting, the model performs well on training data but poorly on new, unseen data.\n",
    "\n",
    "The training data could be insufficient or contain errors. If there isn't enough data or if the data is noisy, the model may not have enough information to learn the underlying patterns accurately.\n",
    "\n",
    "There might be external factors influencing product sales that the model isn't accounting for. For example, changes in government regulations or unexpected events like natural disasters could impact demand and production in ways the model hasn't been trained to handle.\n",
    "\n",
    "Ways in which we can improve the model performance:\n",
    "\n",
    "Thoroughly research and understand the factors that influence renewable energy production and sales. Include features like solar irradiance, wind speed, temperature, and other weather-related variables that directly impact production. Additionally, consider economic indicators, government policies, and competitor actions that could indirectly influence demand.\n",
    "\n",
    "Experiment with different types of models, such as linear regression, decision trees, support vector machines, or even deep learning models like recurrent neural networks (RNNs) or long short-term memory (LSTM) networks. Each model has different strengths and weaknesses, and some might be better suited to your specific data and problem.\n",
    "\n",
    "Identify and remove outliers that might be skewing the model's learning. This could involve statistical methods like the z-score or interquartile range (IQR) method.\n",
    "\n",
    "Standardize or normalize features to ensure they have similar scales. This can help prevent certain features from dominating the learning process.\n",
    "Consider using ensemble methods that combine the predictions of multiple models to improve overall accuracy.\n",
    "\n",
    "Gather data from external sources, such as weather forecasts, economic reports, and news articles related to renewable energy policies. This can help the model account for factors it might not have been trained on.\n",
    "\n",
    "Continuously monitor the model's performance and update it as needed when external factors change. This might involve retraining the model with new data or adjusting its parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
